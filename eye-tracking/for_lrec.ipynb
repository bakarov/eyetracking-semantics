{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame, read_csv, concat\n",
    "from csv import reader\n",
    "from numpy import mean, array, zeros, errstate, seterr, isfinite\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr\n",
    "from os import path\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from itertools import product\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "seterr(all='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_en = DataFrame.from_csv('data/eye-tracking/eye_tracking_data_en_raw.csv').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_ru = DataFrame.from_csv('data/eye-tracking/eye_tracking_data_ru.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load raw messy corpus proposed by the Laboratory of Neurolinguistics and transform it to a decent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(path.join('data', 'data.csv'), mode='r') as infile:\n",
    "#     corpus_bytes = reader(infile)\n",
    "#     corpus = [i for i in corpus_bytes]\n",
    "\n",
    "# corpus = [i[0].split('\\t') if len(i) == 1 else ''.join(i).split('\\t') for i in corpus]\n",
    "# df = DataFrame(corpus[1:], columns=corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount of unique words in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(df['word.id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate corpus data by unqiue words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_en = df_en.astype('str').groupby('Word_Cleaned').agg({\n",
    "#                             'IA_DWELL_TIME': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_DURATION': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_VISITED_IA_COUNT': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_X': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_Y': ', '.join,\n",
    "#                             'IA_FIRST_FIX_PROGRESSIVE': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_TIME': ', '.join,\n",
    "#                             'IA_FIRST_RUN_DWELL_TIME': ', '.join,\n",
    "#                             'IA_FIRST_RUN_START_TIME': ', '.join,\n",
    "#                             'IA_FIRST_RUN_END_TIME': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_TIME': ', '.join,\n",
    "#                             'IA_FIRST_RUN_FIXATION_COUNT': ', '.join,\n",
    "#                             'IA_DWELL_TIME': ', '.join,\n",
    "#                             'IA_FIXATION_COUNT': ', '.join,\n",
    "#                             'IA_RUN_COUNT': ', '.join,\n",
    "#                             'IA_SKIP': ', '.join,\n",
    "#                             'IA_REGRESSION_IN': ', '.join,\n",
    "#                             'IA_REGRESSION_IN_COUNT': ', '.join,\n",
    "#                             'IA_REGRESSION_OUT': ', '.join,\n",
    "#                             'IA_REGRESSION_OUT_COUNT': ', '.join,\n",
    "#                             'IA_REGRESSION_OUT_FULL': ', '.join,\n",
    "#                             'IA_REGRESSION_OUT_FULL_COUNT': ', '.join,\n",
    "#                             'IA_REGRESSION_PATH_DURATION': ', '.join,\n",
    "#                             'IA_REGRESSION_OUT_FULL_COUNT': ', '.join,\n",
    "#                             'IA_REGRESSION_PATH_DURATION': ', '.join,\n",
    "#                             'IA_FIRST_SACCADE_AMPLITUDE': ', '.join,\n",
    "#                             'IA_FIRST_SACCADE_ANGLE': ', '.join,\n",
    "#                             'IA_FIRST_SACCADE_END_TIME': ', '.join,\n",
    "#                             'IA_FIRST_SACCADE_START_TIME': ', '.join\n",
    "#                         }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = df.groupby('Lemma').agg({\n",
    "#                             'average.accuracy': ', '.join,\n",
    "#                             'IA_DWELL_TIME': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_DURATION': ', '.join,\n",
    "#                             'IA_FIRST_RUN_DWELL_TIME': ', '.join,\n",
    "#                             'IA_FIRST_RUN_FIXATION_COUNT': ', '.join,\n",
    "#                             'IA_FIXATION_COUNT': ', '.join,\n",
    "#                             'IA_LEGAL': ', '.join,\n",
    "#                             'IA_REGRESSION_IN': ', '.join,\n",
    "#                             'IA_REGRESSION_OUT_FULL': ', '.join,\n",
    "#                             'IA_REGRESSION_PATH_DURATION': ', '.join,\n",
    "#                             'IA_SECOND_RUN_DWELL_TIME': ', '.join,\n",
    "#                             'ao': ', '.join,\n",
    "#                             'IA_SELECTIVE_REGRESSION_PATH_DURATION': ', '.join,\n",
    "#                             'IA_SKIP': ', '.join,\n",
    "#                             'IA_SPILLOVER': ', '.join,\n",
    "#                             'landing': ', '.join,\n",
    "#                             'dir': ', '.join,\n",
    "#                             'fixated.letter': ', '.join,\n",
    "#                             'one_fix': ', '.join,\n",
    "#                             'twoplus_fix': ', '.join,\n",
    "#                             'logit.acc': ', '.join,\n",
    "#                         }).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ru.to_csv('data/eye-tracking/eye_tracking_data_ru.csv')\n",
    "df_ru = DataFrame.from_csv('data/eye-tracking/eye_tracking_data_ru.csv').fillna(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_en[7:].reset_index(drop=True).to_csv('data/eye-tracking/eye_tracking_data_en.csv')\n",
    "df_en = DataFrame.from_csv('data/eye-tracking/eye_tracking_data_en.csv').fillna(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ru_en_dict = DataFrame.from_csv('data/dictionaries/en-ru-dict.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean value of aggregated values for each word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_mean(df, na):\n",
    "    for column in df:\n",
    "        for i in range(len(df)):\n",
    "            try:\n",
    "                values = list(filter(lambda a: a != na, [a.strip() for a in df[column][i].split(',')]))\n",
    "                if not values:\n",
    "                    df[column][i] = 0\n",
    "                    continue\n",
    "                df[column][i] = mean([float(a) for a in values])\n",
    "            except ValueError: # if column is a column of words\n",
    "                continue \n",
    "            except FloatingPointError: # if all are NA\n",
    "                df[column][i] = 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_en = calculate_mean(df_en, 'nan')\n",
    "df_ru = calculate_mean(df_ru, 'NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and processing dataset of human judgements of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sim_dataset(name, embeddings, verbose=False, rusvectores=False):\n",
    "    df = read_csv(name)\n",
    "    old_len = len(df)\n",
    "    for i, m in df.iterrows():\n",
    "        if not rusvectores:\n",
    "            if not m['word1'] in embeddings or not m['word2'] in embeddings:\n",
    "                df.drop(i, inplace=True)\n",
    "        else:\n",
    "            if not add_pos_tag(m['word1']) in embeddings or not add_pos_tag(m['word2']) in embeddings:\n",
    "                df.drop(i, inplace=True)\n",
    "    if verbose:\n",
    "        print('Percent of dropped = {:2.1f}%, amount of remanining words = {}'.format((old_len - len(df))/old_len*100, len(df)))\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_en_sim_dataset(name, embeddings, verbose=False, rusvectores=False):\n",
    "    df = read_csv(name)\n",
    "    old_len = len(df)\n",
    "    for i, m in df.iterrows():\n",
    "        try:\n",
    "            if not ru_en_dict[ru_en_dict['ru'] == m['word1']].iloc[0, 0] in embeddings or not ru_en_dict[ru_en_dict['ru'] == m['word2']].iloc[0, 0] in embeddings:\n",
    "                df.drop(i, inplace=True)\n",
    "        except IndexError:\n",
    "            df.drop(i, inplace=True)\n",
    "    if verbose:\n",
    "        print('Percent of dropped = {:2.1f}%, amount of remanining words = {}'.format((old_len - len(df))/old_len*100, len(df)))\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_ru_sim_dataset(name, embeddings, verbose=False, rusvectores=False):\n",
    "    df = read_csv(name)\n",
    "    old_len = len(df)\n",
    "    for i, m in df.iterrows():\n",
    "        try:\n",
    "            if not add_pos_tag(ru_en_dict[ru_en_dict['en'] == m['word1']].iloc[0, 1]) in embeddings or not add_pos_tag(ru_en_dict[ru_en_dict['en'] == m['word2']].iloc[0, 1]) in embeddings:\n",
    "                df.drop(i, inplace=True)\n",
    "        except IndexError:\n",
    "            df.drop(i, inplace=True)\n",
    "    if verbose:\n",
    "        print('Percent of dropped = {:2.1f}%, amount of remanining words = {}'.format((old_len - len(df))/old_len*100, len(df)))\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sims_dataset(dataset, embeddings, rusvectores=False):\n",
    "    sims = zeros(shape=len(dataset), dtype='float32')\n",
    "    for i, m in dataset.iterrows():\n",
    "        if not rusvectores:\n",
    "            sims[i] = 1 - cosine(embeddings[m['word1']], embeddings[m['word2']])\n",
    "        else:\n",
    "            sims[i] = 1 - cosine(embeddings[add_pos_tag(m['word1'])], embeddings[add_pos_tag(m['word2'])])\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_en_sims_dataset(dataset, embeddings, rusvectores=False):\n",
    "    sims = []\n",
    "    for i, m in dataset.iterrows():\n",
    "        \n",
    "        try:\n",
    "            sims.append(1 - cosine(embeddings[ru_en_dict[ru_en_dict['ru'] == m['word1']].iloc[0, 0]], embeddings[ru_en_dict[ru_en_dict['ru'] == m['word2']].iloc[0, 0]]))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return array(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_ru_sims_dataset(dataset, embeddings, rusvectores=False):\n",
    "    sims = []\n",
    "    for i, m in dataset.iterrows():\n",
    "        try:\n",
    "            sims.append(1 - cosine(embeddings[add_pos_tag(ru_en_dict[ru_en_dict['en'] == m['word1']].iloc[0, 1])], \n",
    "                        embeddings[add_pos_tag(ru_en_dict[ru_en_dict['en'] == m['word2']].iloc[0, 1])]))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return array(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_mapped_datasets_en_ru(dataset_en, dataset_ru):\n",
    "    sims_en = []\n",
    "    sims_ru = []\n",
    "    for i, m in dataset_en.iterrows():\n",
    "        try:\n",
    "            sims_ru.append(dataset_ru[(dataset_ru['word1'] == morph.parse(ru_en_dict[ru_en_dict['en'] == m['word1']].iloc[0, 1])[0].normal_form) \n",
    "                & (dataset_ru['word2'] == morph.parse(ru_en_dict[ru_en_dict['en'] == m['word2']].iloc[0, 1])[0].normal_form)].iloc[0, 2])\n",
    "            sims_en.append(m['similarity'])\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return array(sims_en), array(sims_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_mapped_datasets_ru_en(dataset_ru, dataset_en):\n",
    "    sims_ru = []\n",
    "    sims_en = []\n",
    "    for i, m in dataset_ru.iterrows():\n",
    "        try:\n",
    "            sims_en.append(dataset_ru[(dataset_ru['word1'] == ru_en_dict[ru_en_dict['ru'] == m['word1']].iloc[0, 0]) \n",
    "                & (dataset_ru['word2'] == ru_en_dict[ru_en_dict['ru'] == m['word2']].iloc[0, 0])].iloc[0, 2])\n",
    "            sims_ru.append(m['sim'])\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return array(sims_en), array(sims_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_word2vec_dataset(dataset, model):\n",
    "    sims = zeros(shape=len(dataset), dtype='float32')\n",
    "    for i, m in dataset.iterrows():\n",
    "        sims[i] = 1 - cosine(model[add_pos_tag(m['word1'])], model[add_pos_tag(m['word2'])])\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_word2vec_eye_dataset(dataset, model, lang='ru'):\n",
    "    pairs = list(product(dataset, repeat=2))\n",
    "    sims_w2v = zeros(shape=len(pairs), dtype='float32')\n",
    "    sims_eye = zeros(shape=len(pairs), dtype='float32')\n",
    "    for i in enumerate(pairs):\n",
    "        try:\n",
    "            if lang == 'en':\n",
    "                sims_w2v[i[0]] = 1 - cosine(model[i[1][0]], model[i[1][1]])\n",
    "                sims_eye[i[0]] = 1 - cosine(dataset[i[1][0]], dataset[i[1][1]])\n",
    "            else: # add pos tags\n",
    "                sims_w2v[i[0]] = 1 - cosine(model[add_pos_tag(i[1][0])], model[add_pos_tag(i[1][1])])\n",
    "                sims_eye[i[0]] = 1 - cosine(dataset[i[1][0]], dataset[i[1][1]])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return sims_w2v, sims_eye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Word2Vec-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rusvectores = KeyedVectors.load_word2vec_format(path.join('..', '..', 'word2vec-models', 'ruwikiruscorpora.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "googlenews = KeyedVectors.load_word2vec_format(path.join('..',  '..', 'word2vec-models', 'google-news.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_pos_tag(word):\n",
    "    tag = str(morph.parse(word)[0].tag.POS)\n",
    "    if tag == 'ADJF':\n",
    "        tag = 'ADJ'\n",
    "    elif tag == 'INFN':\n",
    "        tag = 'VERB'\n",
    "    if word == 'объем': \n",
    "        tag = 'NOUN'\n",
    "    if word == 'струя':\n",
    "        tag = 'NOUN'\n",
    "    if word == 'чай':\n",
    "        tag = 'NOUN'\n",
    "    if word == 'два':\n",
    "        word = 'двадцать'\n",
    "        tag = 'NUM'\n",
    "    return '{}_{}'.format(word, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eye_embeddings_en = {}\n",
    "eye_embeddings_en_ru = {}\n",
    "\n",
    "for i, k in df_en.iloc[:,0:len(df_en.columns)].iterrows():\n",
    "    try:\n",
    "        eye_embeddings_en_ru[ru_en_dict[ru_en_dict['en'] == k['Word_Cleaned']].iloc[0, 1]] = array(k[1:].values)\n",
    "        eye_embeddings_en[k['Word_Cleaned']] = array(k[1:].values)\n",
    "    except IndexError: # no such word in the en-ru dict\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eye_embeddings_ru = {}\n",
    "eye_embeddings_ru_en = {}\n",
    "\n",
    "for i, k in df_ru.iloc[:,0:len(df_ru.columns)].iterrows():\n",
    "    try:\n",
    "        eye_embeddings_ru_en[ru_en_dict[ru_en_dict['ru'] == k['Lemma']].iloc[0, 0]] = array(k[1:].values)\n",
    "        eye_embeddings_ru[k['Lemma']] = array(k[1:].values)\n",
    "    except IndexError: # no such word in the en-ru dict\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "russian_word_similarity = [\n",
    "                ('../../evaluation/ru/word-similarity/simlex999.csv', 'RuSimLex999'),\n",
    "                ('../../evaluation/ru/word-similarity/hj.csv', 'HJ: Human Judgements of Word Pairs')\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_word_similarity = [\n",
    "                ('../../evaluation/en/word-similarity/simverb-3500.csv', 'SimVerb-3500'),\n",
    "                ('../../evaluation/en/word-similarity/men.csv', 'MEN'),\n",
    "                ('../../evaluation/en/word-similarity/rw.csv', 'Rare Word'),\n",
    "                ('../../evaluation/en/word-similarity/simlex999.csv', 'SimLex999'),\n",
    "                ('../../evaluation/en/word-similarity/mturk-771.csv', 'MTurk-771'),\n",
    "                ('../../evaluation/en/word-similarity/semeval17.csv', 'SemEval-2017'),\n",
    "                ('../../evaluation/en/word-similarity/wordsim353-rel.csv', 'WordSim353 Relatedness'),\n",
    "                ('../../evaluation/en/word-similarity/wordsim353-sim.csv', 'WordSim353 Similarity'),\n",
    "                ('../../evaluation/en/word-similarity/mturk-287.csv', 'MTurk-287'),\n",
    "                ('../../evaluation/en/word-similarity/verb-143.csv', 'Verb-143'),\n",
    "                ('../../evaluation/en/word-similarity/yp-130.csv', 'YP-130'),\n",
    "                ('../../evaluation/en/word-similarity/rg-65.csv', 'RG-65'),\n",
    "                ('../../evaluation/en/word-similarity/mc-30.csv', 'MC-30'),\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: RuSimLex999\n",
      "Percent of dropped = 99.1%, amount of remanining words = 9\n",
      "Correlation with Russian human judgements of Russian gaze vectors: 0.47 (0.21)\n",
      "\n",
      "Percent of dropped = 98.7%, amount of remanining words = 13\n",
      "Correlation with Russian human judgements of English gaze vectors: -0.40 (0.17)\n",
      "=================================\n",
      "\n",
      "Dataset: HJ: Human Judgements of Word Pairs\n",
      "Percent of dropped = 97.0%, amount of remanining words = 12\n",
      "Correlation with Russian human judgements of Russian gaze vectors: 0.63 (0.03)\n",
      "\n",
      "Percent of dropped = 97.2%, amount of remanining words = 11\n",
      "Correlation with Russian human judgements of English gaze vectors: -0.31 (0.36)\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, printed_name in russian_word_similarity:\n",
    "        print('Dataset: {}'.format(printed_name))\n",
    "        \n",
    "        dataset = load_sim_dataset(name, eye_embeddings_ru, True)\n",
    "        eye_sims_ru = make_sims_dataset(dataset, eye_embeddings_ru)\n",
    "        print('Correlation with Russian human judgements of Russian gaze vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(eye_sims_ru, dataset.similarity)))\n",
    "        print()\n",
    "        \n",
    "        dataset = load_sim_dataset(name, eye_embeddings_en_ru, True)\n",
    "        eye_sims_en = make_sims_dataset(dataset, eye_embeddings_en_ru)\n",
    "        print('Correlation with Russian human judgements of English gaze vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(eye_sims_en, dataset.similarity)))\n",
    "        print('=================================')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimVerb-3500\n",
      "Percent of dropped = 97.9%, amount of remanining words = 72\n",
      "Correlation with English human judgements of English gaze vectors: 0.14 (0.24)\n",
      "\n",
      "Percent of dropped = 99.6%, amount of remanining words = 13\n",
      "Correlation with English human judgements of Russian gaze vectors: 0.27 (0.37)\n",
      "=================================\n",
      "MEN\n",
      "Percent of dropped = 100.0%, amount of remanining words = 0\n",
      "Correlation with English human judgements of English gaze vectors: nan (nan)\n",
      "\n",
      "Percent of dropped = 100.0%, amount of remanining words = 0\n",
      "Correlation with English human judgements of Russian gaze vectors: nan (nan)\n",
      "=================================\n",
      "Rare Word\n",
      "Percent of dropped = 99.9%, amount of remanining words = 2\n",
      "Correlation with English human judgements of English gaze vectors: -1.00 (nan)\n",
      "\n",
      "Percent of dropped = 100.0%, amount of remanining words = 1\n",
      "Correlation with English human judgements of Russian gaze vectors: nan (nan)\n",
      "=================================\n",
      "SimLex999\n",
      "Percent of dropped = 96.0%, amount of remanining words = 40\n",
      "Correlation with English human judgements of English gaze vectors: -0.10 (0.54)\n",
      "\n",
      "Percent of dropped = 99.2%, amount of remanining words = 8\n",
      "Correlation with English human judgements of Russian gaze vectors: 0.14 (0.74)\n",
      "=================================\n",
      "MTurk-771\n",
      "Percent of dropped = 97.5%, amount of remanining words = 19\n",
      "Correlation with English human judgements of English gaze vectors: 0.25 (0.29)\n",
      "\n",
      "Percent of dropped = 99.0%, amount of remanining words = 8\n",
      "Correlation with English human judgements of Russian gaze vectors: 0.50 (0.21)\n",
      "=================================\n",
      "SemEval-2017\n",
      "Percent of dropped = 98.8%, amount of remanining words = 6\n",
      "Correlation with English human judgements of English gaze vectors: 0.60 (0.21)\n",
      "\n",
      "Percent of dropped = 99.8%, amount of remanining words = 1\n",
      "Correlation with English human judgements of Russian gaze vectors: nan (nan)\n",
      "=================================\n",
      "WordSim353 Relatedness\n",
      "Percent of dropped = 95.7%, amount of remanining words = 11\n",
      "Correlation with English human judgements of English gaze vectors: 0.05 (0.89)\n",
      "\n",
      "Percent of dropped = 97.6%, amount of remanining words = 6\n",
      "Correlation with English human judgements of Russian gaze vectors: 0.52 (0.29)\n",
      "=================================\n",
      "WordSim353 Similarity\n",
      "Percent of dropped = 97.5%, amount of remanining words = 5\n",
      "Correlation with English human judgements of English gaze vectors: 0.60 (0.28)\n",
      "\n",
      "Percent of dropped = 99.5%, amount of remanining words = 1\n",
      "Correlation with English human judgements of Russian gaze vectors: nan (nan)\n",
      "=================================\n",
      "MTurk-287\n",
      "Percent of dropped = 99.3%, amount of remanining words = 2\n",
      "Correlation with English human judgements of English gaze vectors: -1.00 (nan)\n",
      "\n",
      "Percent of dropped = 100.0%, amount of remanining words = 0\n",
      "Correlation with English human judgements of Russian gaze vectors: nan (nan)\n",
      "=================================\n",
      "Verb-143\n",
      "Percent of dropped = 100.0%, amount of remanining words = 0\n",
      "Correlation with English human judgements of English gaze vectors: nan (nan)\n",
      "\n",
      "Percent of dropped = 100.0%, amount of remanining words = 0\n",
      "Correlation with English human judgements of Russian gaze vectors: nan (nan)\n",
      "=================================\n",
      "YP-130\n",
      "Percent of dropped = 100.0%, amount of remanining words = 0\n",
      "Correlation with English human judgements of English gaze vectors: nan (nan)\n",
      "\n",
      "Percent of dropped = 100.0%, amount of remanining words = 0\n",
      "Correlation with English human judgements of Russian gaze vectors: nan (nan)\n",
      "=================================\n",
      "RG-65\n",
      "Percent of dropped = 100.0%, amount of remanining words = 0\n",
      "Correlation with English human judgements of English gaze vectors: nan (nan)\n",
      "\n",
      "Percent of dropped = 100.0%, amount of remanining words = 0\n",
      "Correlation with English human judgements of Russian gaze vectors: nan (nan)\n",
      "=================================\n",
      "MC-30\n",
      "Percent of dropped = 100.0%, amount of remanining words = 0\n",
      "Correlation with English human judgements of English gaze vectors: nan (nan)\n",
      "\n",
      "Percent of dropped = 100.0%, amount of remanining words = 0\n",
      "Correlation with English human judgements of Russian gaze vectors: nan (nan)\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "for name, printed_name in english_word_similarity:\n",
    "        print('{}'.format(printed_name))\n",
    "        \n",
    "        dataset = load_sim_dataset(name, eye_embeddings_en, True)\n",
    "        eye_sims_en = make_sims_dataset(dataset, eye_embeddings_en)\n",
    "        print('Correlation with English human judgements of English gaze vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(eye_sims_en, dataset.similarity)))\n",
    "        print()\n",
    "        \n",
    "        dataset = load_sim_dataset(name, eye_embeddings_ru_en, True)\n",
    "        eye_sims_ru = make_sims_dataset(dataset, eye_embeddings_ru_en)\n",
    "        print('Correlation with English human judgements of Russian gaze vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(eye_sims_ru, dataset.similarity)))\n",
    "        print('=================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'raise', 'invalid': 'raise', 'over': 'raise', 'under': 'raise'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seterr(all='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with Russian gaze vectors of Russian Word2Vec: 0.06 (0.00)\n",
      "Correlation with English gaze vectors of Russian Word2Vec: 0.09 (0.00)\n"
     ]
    }
   ],
   "source": [
    "print('Correlation with Russian gaze vectors of Russian Word2Vec: {:0.2f} ({:0.2f})'.format(*spearmanr(*make_word2vec_eye_dataset(eye_embeddings_ru, rusvectores))))\n",
    "print('Correlation with English gaze vectors of Russian Word2Vec: {:0.2f} ({:0.2f})'.format(*spearmanr(*make_word2vec_eye_dataset(eye_embeddings_en_ru, rusvectores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with Russian gaze vectors of English Word2Vec: 0.14 (0.00)\n",
      "Correlation with English gaze vectors of English Word2Vec: 0.09 (0.00)\n"
     ]
    }
   ],
   "source": [
    "print('Correlation with Russian gaze vectors of English Word2Vec: {:0.2f} ({:0.2f})'.format(*spearmanr(*make_word2vec_eye_dataset(eye_embeddings_ru_en, googlenews, 'en'))))\n",
    "print('Correlation with English gaze vectors of English Word2Vec: {:0.2f} ({:0.2f})'.format(*spearmanr(*make_word2vec_eye_dataset(eye_embeddings_en, googlenews, 'en'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Dataset: SimVerb-3500\n",
      "Russian Dataset: RuSimLex999\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: SimVerb-3500\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: MEN\n",
      "Russian Dataset: RuSimLex999\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: MEN\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: Rare Word\n",
      "Russian Dataset: RuSimLex999\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: Rare Word\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: SimLex999\n",
      "Russian Dataset: RuSimLex999\n",
      "Correlation of English human judgements with Russian human judgements: 0.85 (0.00)\n",
      "\n",
      "English Dataset: SimLex999\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: MTurk-771\n",
      "Russian Dataset: RuSimLex999\n",
      "Correlation of English human judgements with Russian human judgements: 0.00 (1.00)\n",
      "\n",
      "English Dataset: MTurk-771\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: SemEval-2017\n",
      "Russian Dataset: RuSimLex999\n",
      "Correlation of English human judgements with Russian human judgements: 1.00 (nan)\n",
      "\n",
      "English Dataset: SemEval-2017\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: WordSim353 Relatedness\n",
      "Russian Dataset: RuSimLex999\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: WordSim353 Relatedness\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "Correlation of English human judgements with Russian human judgements: 0.81 (0.00)\n",
      "\n",
      "English Dataset: WordSim353 Similarity\n",
      "Russian Dataset: RuSimLex999\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: WordSim353 Similarity\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "Correlation of English human judgements with Russian human judgements: 0.85 (0.00)\n",
      "\n",
      "English Dataset: MTurk-287\n",
      "Russian Dataset: RuSimLex999\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: MTurk-287\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: Verb-143\n",
      "Russian Dataset: RuSimLex999\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: Verb-143\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: YP-130\n",
      "Russian Dataset: RuSimLex999\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: YP-130\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: RG-65\n",
      "Russian Dataset: RuSimLex999\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: RG-65\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "Correlation of English human judgements with Russian human judgements: 0.98 (0.00)\n",
      "\n",
      "English Dataset: MC-30\n",
      "Russian Dataset: RuSimLex999\n",
      "Correlation of English human judgements with Russian human judgements: nan (nan)\n",
      "\n",
      "English Dataset: MC-30\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "Correlation of English human judgements with Russian human judgements: 1.00 (0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for en_dataset, printed_name_en in english_word_similarity:\n",
    "    for ru_dataset, printed_name_ru in russian_word_similarity:\n",
    "        print('English Dataset: {}'.format(printed_name_en))\n",
    "        print('Russian Dataset: {}'.format(printed_name_ru))\n",
    "        print('Correlation of English human judgements with Russian human judgements: {:0.2f} ({:0.2f})'.\n",
    "              format(*spearmanr(*make_mapped_datasets_en_ru(\n",
    "                  DataFrame.from_csv(en_dataset), DataFrame.from_csv(ru_dataset)))))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian Dataset: RuSimLex999\n",
      "English Dataset: SimVerb-3500\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: RuSimLex999\n",
      "English Dataset: MEN\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: RuSimLex999\n",
      "English Dataset: Rare Word\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: RuSimLex999\n",
      "English Dataset: SimLex999\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: RuSimLex999\n",
      "English Dataset: MTurk-771\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: RuSimLex999\n",
      "English Dataset: SemEval-2017\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: RuSimLex999\n",
      "English Dataset: WordSim353 Relatedness\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: RuSimLex999\n",
      "English Dataset: WordSim353 Similarity\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: RuSimLex999\n",
      "English Dataset: MTurk-287\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: RuSimLex999\n",
      "English Dataset: Verb-143\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: RuSimLex999\n",
      "English Dataset: YP-130\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: RuSimLex999\n",
      "English Dataset: RG-65\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: RuSimLex999\n",
      "English Dataset: MC-30\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "English Dataset: SimVerb-3500\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "English Dataset: MEN\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "English Dataset: Rare Word\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "English Dataset: SimLex999\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "English Dataset: MTurk-771\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "English Dataset: SemEval-2017\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "English Dataset: WordSim353 Relatedness\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "English Dataset: WordSim353 Similarity\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "English Dataset: MTurk-287\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "English Dataset: Verb-143\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "English Dataset: YP-130\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "English Dataset: RG-65\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n",
      "Russian Dataset: HJ: Human Judgements of Word Pairs\n",
      "English Dataset: MC-30\n",
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ru_dataset, printed_name_ru in russian_word_similarity:\n",
    "        for en_dataset, printed_name_en in english_word_similarity:\n",
    "            print('Russian Dataset: {}'.format(printed_name_ru))\n",
    "            print('English Dataset: {}'.format(printed_name_en))\n",
    "            print('Correlation of Russian human judgements with English human judgements: {:0.2f} ({:0.2f})'.\n",
    "                  format(*spearmanr(*make_mapped_datasets_ru_en(\n",
    "                      DataFrame.from_csv(ru_dataset), DataFrame.from_csv(en_dataset)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: RuSimLex999\n",
      "Percent of dropped = 2.6%, amount of remanining words = 973\n",
      "Percent of dropped = 43.0%, amount of remanining words = 555\n",
      "Correlation with Russian human judgements of English word2vec vectors: 0.24 (0.00)\n",
      "\n",
      "Correlation with Russian human judgements of Russian word2vec vectors: 0.28 (0.00)\n",
      "\n",
      "=================================\n",
      "Dataset: HJ: Human Judgements of Word Pairs\n",
      "Percent of dropped = 5.8%, amount of remanining words = 375\n",
      "Percent of dropped = 21.1%, amount of remanining words = 296\n",
      "Correlation with Russian human judgements of English word2vec vectors: 0.58 (0.00)\n",
      "\n",
      "Correlation with Russian human judgements of Russian word2vec vectors: 0.69 (0.00)\n",
      "\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "for name, printed_name in russian_word_similarity:\n",
    "        print('Dataset: {}'.format(printed_name))\n",
    "        \n",
    "        dataset = load_sim_dataset(name, rusvectores, True, True)\n",
    "        dataset.to_csv('temp.csv')\n",
    "        dataset = load_en_sim_dataset('temp.csv', googlenews, True)\n",
    "        \n",
    "        w2v_en = make_en_sims_dataset(dataset, googlenews)\n",
    "        print('Correlation with Russian human judgements of English word2vec vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(w2v_en, dataset.similarity)))\n",
    "        print()\n",
    "       \n",
    "        w2v_ru = make_sims_dataset(dataset, rusvectores, rusvectores=True)\n",
    "        print('Correlation with Russian human judgements of Russian word2vec vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(w2v_ru, dataset.similarity)))\n",
    "        print()\n",
    "        print('=================================')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: SimVerb-3500\n",
      "Percent of dropped = 81.7%, amount of remanining words = 639\n",
      "Percent of dropped = 0.0%, amount of remanining words = 639\n",
      "Correlation with English human judgements of Russian word2vec vectors: 0.12 (0.00)\n",
      "\n",
      "Correlation with Enlglish human judgements of English word2vec vectors: 0.38 (0.00)\n",
      "=================================\n",
      "\n",
      "Dataset: MEN\n",
      "Percent of dropped = 100.0%, amount of remanining words = 0\n",
      "All data is dropped ~\n",
      "Dataset: Rare Word\n",
      "Percent of dropped = 95.5%, amount of remanining words = 91\n",
      "Percent of dropped = 2.2%, amount of remanining words = 89\n",
      "Correlation with English human judgements of Russian word2vec vectors: 0.52 (0.00)\n",
      "\n",
      "Correlation with Enlglish human judgements of English word2vec vectors: 0.53 (0.00)\n",
      "=================================\n",
      "\n",
      "Dataset: SimLex999\n",
      "Percent of dropped = 59.9%, amount of remanining words = 401\n",
      "Percent of dropped = 0.0%, amount of remanining words = 401\n",
      "Correlation with English human judgements of Russian word2vec vectors: 0.33 (0.00)\n",
      "\n",
      "Correlation with Enlglish human judgements of English word2vec vectors: 0.36 (0.00)\n",
      "=================================\n",
      "\n",
      "Dataset: MTurk-771\n",
      "Percent of dropped = 54.5%, amount of remanining words = 351\n",
      "Percent of dropped = 0.0%, amount of remanining words = 351\n",
      "Correlation with English human judgements of Russian word2vec vectors: 0.46 (0.00)\n",
      "\n",
      "Correlation with Enlglish human judgements of English word2vec vectors: 0.70 (0.00)\n",
      "=================================\n",
      "\n",
      "Dataset: SemEval-2017\n",
      "Percent of dropped = 69.0%, amount of remanining words = 155\n",
      "Percent of dropped = 0.6%, amount of remanining words = 154\n",
      "Correlation with English human judgements of Russian word2vec vectors: 0.59 (0.00)\n",
      "\n",
      "Correlation with Enlglish human judgements of English word2vec vectors: 0.74 (0.00)\n",
      "=================================\n",
      "\n",
      "Dataset: WordSim353 Relatedness\n",
      "Percent of dropped = 57.7%, amount of remanining words = 107\n",
      "Percent of dropped = 0.0%, amount of remanining words = 107\n",
      "Correlation with English human judgements of Russian word2vec vectors: 0.46 (0.00)\n",
      "\n",
      "Correlation with Enlglish human judgements of English word2vec vectors: 0.65 (0.00)\n",
      "=================================\n",
      "\n",
      "Dataset: WordSim353 Similarity\n",
      "Percent of dropped = 60.3%, amount of remanining words = 81\n",
      "Percent of dropped = 0.0%, amount of remanining words = 81\n",
      "Correlation with English human judgements of Russian word2vec vectors: 0.67 (0.00)\n",
      "\n",
      "Correlation with Enlglish human judgements of English word2vec vectors: 0.79 (0.00)\n",
      "=================================\n",
      "\n",
      "Dataset: MTurk-287\n",
      "Percent of dropped = 72.1%, amount of remanining words = 80\n",
      "Percent of dropped = 2.5%, amount of remanining words = 78\n",
      "Correlation with English human judgements of Russian word2vec vectors: 0.60 (0.00)\n",
      "\n",
      "Correlation with Enlglish human judgements of English word2vec vectors: 0.76 (0.00)\n",
      "=================================\n",
      "\n",
      "Dataset: Verb-143\n",
      "Percent of dropped = 85.4%, amount of remanining words = 19\n",
      "Percent of dropped = 0.0%, amount of remanining words = 19\n",
      "Correlation with English human judgements of Russian word2vec vectors: 0.29 (0.22)\n",
      "\n",
      "Correlation with Enlglish human judgements of English word2vec vectors: 0.67 (0.00)\n",
      "=================================\n",
      "\n",
      "Dataset: YP-130\n",
      "Percent of dropped = 83.8%, amount of remanining words = 21\n",
      "Percent of dropped = 0.0%, amount of remanining words = 21\n",
      "Correlation with English human judgements of Russian word2vec vectors: 0.29 (0.20)\n",
      "\n",
      "Correlation with Enlglish human judgements of English word2vec vectors: 0.64 (0.00)\n",
      "=================================\n",
      "\n",
      "Dataset: RG-65\n",
      "Percent of dropped = 61.5%, amount of remanining words = 25\n",
      "Percent of dropped = 0.0%, amount of remanining words = 25\n",
      "Correlation with English human judgements of Russian word2vec vectors: 0.43 (0.03)\n",
      "\n",
      "Correlation with Enlglish human judgements of English word2vec vectors: 0.77 (0.00)\n",
      "=================================\n",
      "\n",
      "Dataset: MC-30\n",
      "Percent of dropped = 50.0%, amount of remanining words = 15\n",
      "Percent of dropped = 0.0%, amount of remanining words = 15\n",
      "Correlation with English human judgements of Russian word2vec vectors: 0.46 (0.08)\n",
      "\n",
      "Correlation with Enlglish human judgements of English word2vec vectors: 0.79 (0.00)\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, printed_name in english_word_similarity:\n",
    "        print('Dataset: {}'.format(printed_name))\n",
    "        \n",
    "        dataset = load_ru_sim_dataset(name, rusvectores, True, rusvectores=True)\n",
    "        dataset.to_csv('temp.csv')\n",
    "        try:\n",
    "            dataset = load_sim_dataset('temp.csv', googlenews, True)\n",
    "        except ZeroDivisionError:\n",
    "            print('All data is dropped ~')\n",
    "            continue\n",
    "        \n",
    "        w2v_ru = make_ru_sims_dataset(dataset, rusvectores, rusvectores=True)\n",
    "        print('Correlation with English human judgements of Russian word2vec vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(w2v_ru, dataset.similarity)))\n",
    "        print()\n",
    "        \n",
    "        w2v_en = make_sims_dataset(dataset, googlenews)\n",
    "        print('Correlation with Enlglish human judgements of English word2vec vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(w2v_en, dataset.similarity)))\n",
    "        print('=================================')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
