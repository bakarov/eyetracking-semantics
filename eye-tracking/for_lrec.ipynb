{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'invalid': 'ignore', 'over': 'ignore', 'under': 'ignore'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame, read_csv, concat\n",
    "from csv import reader\n",
    "from numpy import mean, array, zeros, errstate, seterr, isfinite\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr\n",
    "from os import path\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from itertools import product\n",
    "import string\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "morph = MorphAnalyzer()\n",
    "seterr(all='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_en = DataFrame.from_csv('data/eye-tracking/eye_tracking_data_en_raw.csv').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_ru = DataFrame.from_csv('data/eye-tracking/eye_tracking_data_ru.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# geco_raw = DataFrame.from_csv('data/eye-tracking/geco_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load raw messy corpus proposed by the Laboratory of Neurolinguistics and transform it to a decent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(path.join('data', 'data.csv'), mode='r') as infile:\n",
    "#     corpus_bytes = reader(infile)\n",
    "#     corpus = [i for i in corpus_bytes]\n",
    "\n",
    "# corpus = [i[0].split('\\t') if len(i) == 1 else ''.join(i).split('\\t') for i in corpus]\n",
    "# df = DataFrame(corpus[1:], columns=corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount of unique words in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(df['word.id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate corpus data by unqiue words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geco_raw.WORD = geco_raw.WORD.apply(lambda x: x.translate(translator).lower().strip())\n",
    "# geco_raw = geco_raw.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geco_raw = geco_raw.astype('str').groupby('WORD').agg({\n",
    "#                             'TRIAL_TOTAL_READING_TIME': ', '.join,\n",
    "#                             'WORD_ID_WITHIN_TRIAL': ', '.join,\n",
    "#                             'WORD_AVERAGE_FIX_PUPIL_SIZE': ', '.join,\n",
    "#                             'WORD_FIXATION_COUNT': ', '.join,\n",
    "#                             'WORD_FIXATION_%': ', '.join,\n",
    "#                             'WORD_RUN_COUNT': ', '.join,\n",
    "#                             'WORD_FIRST_RUN_START_TIME': ', '.join,\n",
    "#                             'WORD_FIRST_RUN_END_TIME': ', '.join,\n",
    "#                             'WORD_FIRST_RUN_FIXATION_COUNT': ', '.join,\n",
    "#                             'WORD_FIRST_RUN_FIXATION_%': ', '.join,\n",
    "#                             'WORD_GAZE_DURATION': ', '.join,\n",
    "#                             'WORD_SECOND_RUN_START_TIME': ', '.join,\n",
    "#                             'WORD_SECOND_RUN_END_TIME': ', '.join,\n",
    "#                             'WORD_SECOND_RUN_FIXATION_COUNT': ', '.join,\n",
    "#                             'WORD_SECOND_RUN_FIXATION_%': ', '.join,\n",
    "#                             'WORD_THIRD_RUN_START_TIME': ', '.join,\n",
    "#                             'WORD_THIRD_RUN_END_TIME': ', '.join,\n",
    "#                             'WORD_THIRD_RUN_FIXATION_COUNT': ', '.join,\n",
    "#                             'WORD_THIRD_RUN_FIXATION_%': ', '.join,\n",
    "#                             'WORD_FIRST_FIXATION_DURATION': ', '.join,\n",
    "#                             'WORD_FIRST_FIXATION_INDEX': ', '.join,\n",
    "#                             'WORD_FIRST_FIXATION_RUN_INDEX': ', '.join,\n",
    "#                             'WORD_FIRST_FIXATION_TIME': ', '.join,\n",
    "#                             'WORD_FIRST_FIXATION_VISITED_WORD_COUNT': ', '.join,\n",
    "#                             'WORD_FIRST_FIXATION_X': ', '.join,\n",
    "#                             'WORD_FIRST_FIXATION_Y': ', '.join,\n",
    "#                             'WORD_FIRST_FIX_PROGRESSIVE': ', '.join,\n",
    "#                             'WORD_SECOND_FIXATION_DURATION': ', '.join,\n",
    "#                             'WORD_SECOND_FIXATION_RUN': ', '.join,\n",
    "#                             'WORD_SECOND_FIXATION_TIME': ', '.join,\n",
    "#                             'WORD_SECOND_FIXATION_X': ', '.join,\n",
    "#                             'WORD_SECOND_FIXATION_Y': ', '.join,\n",
    "#                             'WORD_THIRD_FIXATION_DURATION': ', '.join,\n",
    "#                             'WORD_THIRD_FIXATION_RUN': ', '.join,\n",
    "#                             'WORD_THIRD_FIXATION_TIME': ', '.join,\n",
    "#                             'WORD_THIRD_FIXATION_X': ', '.join,\n",
    "#                             'WORD_THIRD_FIXATION_Y': ', '.join,\n",
    "#                             'WORD_LAST_FIXATION_DURATION': ', '.join,\n",
    "#                             'WORD_LAST_FIXATION_RUN': ', '.join,\n",
    "#                             'WORD_LAST_FIXATION_TIME': ', '.join,\n",
    "#                             'WORD_LAST_FIXATION_X': ', '.join,\n",
    "#                             'WORD_LAST_FIXATION_Y': ', '.join,\n",
    "#                             'WORD_GO_PAST_TIME': ', '.join,\n",
    "#                             'WORD_SELECTIVE_GO_PAST_TIME': ', '.join,\n",
    "#                             'WORD_TOTAL_READING_TIME': ', '.join,\n",
    "#                             'WORD_TOTAL_READING_TIME_%': ', '.join,\n",
    "#                             'WORD_SPILLOVER': ', '.join,\n",
    "#                             'WORD_SKIP': ', '.join,\n",
    "#                         }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_en = df_en.astype('str').groupby('Word_Cleaned').agg({\n",
    "#                             'IA_DWELL_TIME': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_DURATION': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_VISITED_IA_COUNT': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_X': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_Y': ', '.join,\n",
    "#                             'IA_FIRST_FIX_PROGRESSIVE': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_TIME': ', '.join,\n",
    "#                             'IA_FIRST_RUN_DWELL_TIME': ', '.join,\n",
    "#                             'IA_FIRST_RUN_START_TIME': ', '.join,\n",
    "#                             'IA_FIRST_RUN_END_TIME': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_TIME': ', '.join,\n",
    "#                             'IA_FIRST_RUN_FIXATION_COUNT': ', '.join,\n",
    "#                             'IA_DWELL_TIME': ', '.join,\n",
    "#                             'IA_FIXATION_COUNT': ', '.join,\n",
    "#                             'IA_RUN_COUNT': ', '.join,\n",
    "#                             'IA_SKIP': ', '.join,\n",
    "#                             'IA_REGRESSION_IN': ', '.join,\n",
    "#                             'IA_REGRESSION_IN_COUNT': ', '.join,\n",
    "#                             'IA_REGRESSION_OUT': ', '.join,\n",
    "#                             'IA_REGRESSION_OUT_COUNT': ', '.join,\n",
    "#                             'IA_REGRESSION_OUT_FULL': ', '.join,\n",
    "#                             'IA_REGRESSION_OUT_FULL_COUNT': ', '.join,\n",
    "#                             'IA_REGRESSION_PATH_DURATION': ', '.join,\n",
    "#                             'IA_REGRESSION_OUT_FULL_COUNT': ', '.join,\n",
    "#                             'IA_REGRESSION_PATH_DURATION': ', '.join,\n",
    "#                             'IA_FIRST_SACCADE_AMPLITUDE': ', '.join,\n",
    "#                             'IA_FIRST_SACCADE_ANGLE': ', '.join,\n",
    "#                             'IA_FIRST_SACCADE_END_TIME': ', '.join,\n",
    "#                             'IA_FIRST_SACCADE_START_TIME': ', '.join\n",
    "#                         }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = df.groupby('Lemma').agg({\n",
    "#                             'average.accuracy': ', '.join,\n",
    "#                             'IA_DWELL_TIME': ', '.join,\n",
    "#                             'IA_FIRST_FIXATION_DURATION': ', '.join,\n",
    "#                             'IA_FIRST_RUN_DWELL_TIME': ', '.join,\n",
    "#                             'IA_FIRST_RUN_FIXATION_COUNT': ', '.join,\n",
    "#                             'IA_FIXATION_COUNT': ', '.join,\n",
    "#                             'IA_LEGAL': ', '.join,\n",
    "#                             'IA_REGRESSION_IN': ', '.join,\n",
    "#                             'IA_REGRESSION_OUT_FULL': ', '.join,\n",
    "#                             'IA_REGRESSION_PATH_DURATION': ', '.join,\n",
    "#                             'IA_SECOND_RUN_DWELL_TIME': ', '.join,\n",
    "#                             'ao': ', '.join,\n",
    "#                             'IA_SELECTIVE_REGRESSION_PATH_DURATION': ', '.join,\n",
    "#                             'IA_SKIP': ', '.join,\n",
    "#                             'IA_SPILLOVER': ', '.join,\n",
    "#                             'landing': ', '.join,\n",
    "#                             'dir': ', '.join,\n",
    "#                             'fixated.letter': ', '.join,\n",
    "#                             'one_fix': ', '.join,\n",
    "#                             'twoplus_fix': ', '.join,\n",
    "#                             'logit.acc': ', '.join,\n",
    "#                         }).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_ru.to_csv('data/eye-tracking/eye_tracking_data_ru.csv')\n",
    "df_ru = DataFrame.from_csv('data/eye-tracking/eye_tracking_data_ru.csv').fillna(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_en[7:].reset_index(drop=True).to_csv('data/eye-tracking/eye_tracking_data_en.csv')\n",
    "df_en = DataFrame.from_csv('data/eye-tracking/eye_tracking_data_en.csv').fillna(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# geco_raw.to_csv('data/eye-tracking/geco.csv')\n",
    "geco = DataFrame.from_csv('data/eye-tracking/geco.csv').fillna(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ru_en_dict = DataFrame.from_csv('data/dictionaries/en-ru-dict.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean value of aggregated values for each word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_mean(df, na):\n",
    "    for column in df:\n",
    "        for i in range(len(df)):\n",
    "            try:\n",
    "                values = list(filter(lambda a: a != na, [a.strip() for a in df[column][i].split(',')]))\n",
    "                if not values:\n",
    "                    df[column][i] = 0\n",
    "                    continue\n",
    "                df[column][i] = mean([float(a) for a in values])\n",
    "            except ValueError: # if column is a column of words\n",
    "                continue \n",
    "            except FloatingPointError: # if all are NA\n",
    "                df[column][i] = 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_en = calculate_mean(df_en, 'nan')\n",
    "df_ru = calculate_mean(df_ru, 'NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and processing dataset of human judgements of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sim_dataset(df, embeddings, verbose=False, rusvectores=False):\n",
    "    # df = read_csv(name)\n",
    "    old_len = len(df)\n",
    "    for i, m in df.iterrows():\n",
    "        if not rusvectores:\n",
    "            if not m['word1'] in embeddings or not m['word2'] in embeddings:\n",
    "                df.drop(i, inplace=True)\n",
    "        else:\n",
    "            if not add_pos_tag(m['word1']) in embeddings or not add_pos_tag(m['word2']) in embeddings:\n",
    "                df.drop(i, inplace=True)\n",
    "    if verbose:\n",
    "        print('Percent of dropped = {:2.1f}%, amount of remanining words = {}'.format((old_len - len(df))/old_len*100, len(df)))\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_en_sim_dataset(df, embeddings, verbose=False, rusvectores=False):\n",
    "    # df = read_csv(name)\n",
    "    old_len = len(df)\n",
    "    for i, m in df.iterrows():\n",
    "        try:\n",
    "            if not ru_en_dict[ru_en_dict['ru'] == m['word1']].iloc[0, 0] in embeddings or not ru_en_dict[ru_en_dict['ru'] == m['word2']].iloc[0, 0] in embeddings:\n",
    "                df.drop(i, inplace=True)\n",
    "        except IndexError:\n",
    "            df.drop(i, inplace=True)\n",
    "    if verbose:\n",
    "        print('Percent of dropped = {:2.1f}%, amount of remanining words = {}'.format((old_len - len(df))/old_len*100, len(df)))\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_ru_sim_dataset(df, embeddings, verbose=False, rusvectores=False):\n",
    "    # df = read_csv(name)\n",
    "    old_len = len(df)\n",
    "    for i, m in df.iterrows():\n",
    "        try:\n",
    "            if not add_pos_tag(ru_en_dict[ru_en_dict['en'] == m['word1']].iloc[0, 1]) in embeddings or not add_pos_tag(ru_en_dict[ru_en_dict['en'] == m['word2']].iloc[0, 1]) in embeddings:\n",
    "                df.drop(i, inplace=True)\n",
    "        except IndexError:\n",
    "            df.drop(i, inplace=True)\n",
    "    if verbose:\n",
    "        print('Percent of dropped = {:2.1f}%, amount of remanining words = {}'.format((old_len - len(df))/old_len*100, len(df)))\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sims_dataset(dataset, embeddings, rusvectores=False):\n",
    "    sims = zeros(shape=len(dataset), dtype='float32')\n",
    "    for i, m in dataset.iterrows():\n",
    "        if not rusvectores:\n",
    "            sims[i] = 1 - cosine(embeddings[m['word1']], embeddings[m['word2']])\n",
    "        else:\n",
    "            sims[i] = 1 - cosine(embeddings[add_pos_tag(m['word1'])], embeddings[add_pos_tag(m['word2'])])\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_en_sims_dataset(dataset, embeddings, rusvectores=False):\n",
    "    sims = []\n",
    "    for i, m in dataset.iterrows():\n",
    "        \n",
    "        try:\n",
    "            sims.append(1 - cosine(embeddings[ru_en_dict[ru_en_dict['ru'] == m['word1']].iloc[0, 0]], embeddings[ru_en_dict[ru_en_dict['ru'] == m['word2']].iloc[0, 0]]))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return array(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_ru_sims_dataset(dataset, embeddings, rusvectores=False):\n",
    "    sims = []\n",
    "    for i, m in dataset.iterrows():\n",
    "        try:\n",
    "            sims.append(1 - cosine(embeddings[add_pos_tag(ru_en_dict[ru_en_dict['en'] == m['word1']].iloc[0, 1])], \n",
    "                        embeddings[add_pos_tag(ru_en_dict[ru_en_dict['en'] == m['word2']].iloc[0, 1])]))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return array(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_mapped_datasets_en_ru(dataset_en, dataset_ru):\n",
    "    sims_en = []\n",
    "    sims_ru = []\n",
    "    for i, m in dataset_en.iterrows():\n",
    "        try:\n",
    "            sims_ru.append(dataset_ru[(dataset_ru['word1'] == morph.parse(ru_en_dict[ru_en_dict['en'] == m['word1']].iloc[0, 1])[0].normal_form) \n",
    "                & (dataset_ru['word2'] == morph.parse(ru_en_dict[ru_en_dict['en'] == m['word2']].iloc[0, 1])[0].normal_form)].iloc[0, 2])\n",
    "            sims_en.append(m['similarity'])\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return array(sims_en), array(sims_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_mapped_datasets_ru_en(dataset_ru, dataset_en):\n",
    "    sims_ru = []\n",
    "    sims_en = []\n",
    "    for i, m in dataset_ru.iterrows():\n",
    "        try:\n",
    "            sims_en.append(dataset_ru[(dataset_ru['word1'] == ru_en_dict[ru_en_dict['ru'] == m['word1']].iloc[0, 0]) \n",
    "                & (dataset_ru['word2'] == ru_en_dict[ru_en_dict['ru'] == m['word2']].iloc[0, 0])].iloc[0, 2])\n",
    "            sims_ru.append(m['sim'])\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return array(sims_en), array(sims_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_word2vec_dataset(dataset, model):\n",
    "    sims = zeros(shape=len(dataset), dtype='float32')\n",
    "    for i, m in dataset.iterrows():\n",
    "        sims[i] = 1 - cosine(model[add_pos_tag(m['word1'])], model[add_pos_tag(m['word2'])])\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_word2vec_eye_dataset(dataset, model, lang='ru'):\n",
    "    pairs = list(product(dataset, repeat=2))\n",
    "    sims_w2v = zeros(shape=len(pairs), dtype='float32')\n",
    "    sims_eye = zeros(shape=len(pairs), dtype='float32')\n",
    "    for i in enumerate(pairs):\n",
    "        try:\n",
    "            if lang == 'en':\n",
    "                sims_w2v[i[0]] = 1 - cosine(model[i[1][0]], model[i[1][1]])\n",
    "                sims_eye[i[0]] = 1 - cosine(dataset[i[1][0]], dataset[i[1][1]])\n",
    "            else: # add pos tags\n",
    "                sims_w2v[i[0]] = 1 - cosine(model[add_pos_tag(i[1][0])], model[add_pos_tag(i[1][1])])\n",
    "                sims_eye[i[0]] = 1 - cosine(dataset[i[1][0]], dataset[i[1][1]])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return sims_w2v, sims_eye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Word2Vec-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rusvectores = KeyedVectors.load_word2vec_format(path.join('..', '..', 'word2vec-models', 'ruwikiruscorpora.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "googlenews = KeyedVectors.load_word2vec_format(path.join('..',  '..', 'word2vec-models', 'google-news.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_pos_tag(word):\n",
    "    tag = str(morph.parse(word)[0].tag.POS)\n",
    "    if tag == 'ADJF':\n",
    "        tag = 'ADJ'\n",
    "    elif tag == 'INFN':\n",
    "        tag = 'VERB'\n",
    "    if word == 'объем': \n",
    "        tag = 'NOUN'\n",
    "    if word == 'струя':\n",
    "        tag = 'NOUN'\n",
    "    if word == 'чай':\n",
    "        tag = 'NOUN'\n",
    "    if word == 'два':\n",
    "        word = 'двадцать'\n",
    "        tag = 'NUM'\n",
    "    return '{}_{}'.format(word, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_embeddings_en = {}\n",
    "eye_embeddings_en_ru = {}\n",
    "\n",
    "for i, k in df_en.iloc[:,0:len(df_en.columns)].iterrows():\n",
    "    try:\n",
    "        eye_embeddings_en_ru[ru_en_dict[ru_en_dict['en'] == k['word']].iloc[0, 1]] = array(k[1:].values)\n",
    "        eye_embeddings_en[k['word']] = array(k[1:].values)\n",
    "    except IndexError: # no such word in the en-ru dict\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eye_embeddings_ru = {}\n",
    "eye_embeddings_ru_en = {}\n",
    "\n",
    "for i, k in df_ru.iloc[:,0:len(df_ru.columns)].iterrows():\n",
    "    try:\n",
    "        eye_embeddings_ru_en[ru_en_dict[ru_en_dict['ru'] == k['word']].iloc[0, 0]] = array(k[1:].values)\n",
    "        eye_embeddings_ru[k['word']] = array(k[1:].values)\n",
    "    except IndexError: # no such word in the en-ru dict\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rw = concat([read_csv(_[0]) for _ in russian_word_similarity]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_word_similarity = [\n",
    "                ('../../evaluation/en/word-similarity/simverb-3500.csv', 'SimVerb-3500'),\n",
    "                ('../../evaluation/en/word-similarity/men.csv', 'MEN'),\n",
    "                ('../../evaluation/en/word-similarity/rw.csv', 'Rare Word'),\n",
    "                ('../../evaluation/en/word-similarity/simlex999.csv', 'SimLex999'),\n",
    "                ('../../evaluation/en/word-similarity/mturk-771.csv', 'MTurk-771'),\n",
    "                # ('../../evaluation/en/word-similarity/semeva2017.csv', 'SemEval-2017'),\n",
    "                ('../../evaluation/en/word-similarity/wordsim353-rel.csv', 'WordSim353 Relatedness'),\n",
    "                ('../../evaluation/en/word-similarity/wordsim353-sim.csv', 'WordSim353 Similarity'),\n",
    "                ('../../evaluation/en/word-similarity/mturk-287.csv', 'MTurk-287'),\n",
    "                ('../../evaluation/en/word-similarity/verb-143.csv', 'Verb-143'),\n",
    "                ('../../evaluation/en/word-similarity/yp-130.csv', 'YP-130'),\n",
    "                ('../../evaluation/en/word-similarity/rg-65.csv', 'RG-65'),\n",
    "                ('../../evaluation/en/word-similarity/mc-30.csv', 'MC-30'),\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ew = concat([read_csv(_[0]) for _ in english_word_similarity]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of dropped = 98.5%, amount of remanining words = 21\n",
      "Correlation with Russian human judgements of Russian gaze vectors: 0.57 (0.01)\n",
      "\n",
      "Percent of dropped = 99.6%, amount of remanining words = 6\n",
      "Correlation with Russian human judgements of English gaze vectors: 0.03 (0.96)\n"
     ]
    }
   ],
   "source": [
    "df = rw.copy()\n",
    "dataset = load_sim_dataset(df, eye_embeddings_ru, True)\n",
    "eye_sims_ru = make_sims_dataset(dataset, eye_embeddings_ru)\n",
    "print('Correlation with Russian human judgements of Russian gaze vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(eye_sims_ru, dataset.similarity)))\n",
    "print()\n",
    "\n",
    "df = rw.copy()\n",
    "dataset = load_sim_dataset(df, eye_embeddings_en_ru, True)\n",
    "eye_sims_en = make_sims_dataset(dataset, eye_embeddings_en_ru)\n",
    "print('Correlation with Russian human judgements of English gaze vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(eye_sims_en, dataset.similarity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of dropped = 98.0%, amount of remanining words = 232\n",
      "Correlation with English human judgements of English gaze vectors: -0.03 (0.60)\n",
      "\n",
      "Percent of dropped = 99.7%, amount of remanining words = 37\n",
      "Correlation with English human judgements of Russian gaze vectors: 0.31 (0.06)\n"
     ]
    }
   ],
   "source": [
    "df = ew.copy()\n",
    "dataset = load_sim_dataset(df, eye_embeddings_en, True)\n",
    "eye_sims_en = make_sims_dataset(dataset, eye_embeddings_en)\n",
    "print('Correlation with English human judgements of English gaze vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(eye_sims_en, dataset.similarity)))\n",
    "print()\n",
    "\n",
    "df = ew.copy()\n",
    "dataset = load_sim_dataset(df, eye_embeddings_ru_en, True)\n",
    "eye_sims_ru = make_sims_dataset(dataset, eye_embeddings_ru_en)\n",
    "print('Correlation with English human judgements of Russian gaze vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(eye_sims_ru, dataset.similarity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'raise', 'invalid': 'raise', 'over': 'raise', 'under': 'raise'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seterr(all='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with Russian gaze vectors of Russian Word2Vec: 0.06 (0.00)\n",
      "Correlation with English gaze vectors of Russian Word2Vec: 0.08 (0.00)\n"
     ]
    }
   ],
   "source": [
    "print('Correlation with Russian gaze vectors of Russian Word2Vec: {:0.2f} ({:0.2f})'.format(*spearmanr(*make_word2vec_eye_dataset(eye_embeddings_ru, rusvectores))))\n",
    "print('Correlation with English gaze vectors of Russian Word2Vec: {:0.2f} ({:0.2f})'.format(*spearmanr(*make_word2vec_eye_dataset(eye_embeddings_en_ru, rusvectores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with Russian gaze vectors of English Word2Vec: 0.14 (0.00)\n",
      "Correlation with English gaze vectors of English Word2Vec: 0.09 (0.00)\n"
     ]
    }
   ],
   "source": [
    "print('Correlation with Russian gaze vectors of English Word2Vec: {:0.2f} ({:0.2f})'.format(*spearmanr(*make_word2vec_eye_dataset(eye_embeddings_ru_en, googlenews, 'en'))))\n",
    "print('Correlation with English gaze vectors of English Word2Vec: {:0.2f} ({:0.2f})'.format(*spearmanr(*make_word2vec_eye_dataset(eye_embeddings_en, googlenews, 'en'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of English human judgements with Russian human judgements: -0.03 (0.54)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/defeater/anaconda/lib/python3.6/site-packages/scipy/stats/stats.py:253: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "df_en = ew.copy()\n",
    "df_ru = rw.copy()\n",
    "\n",
    "print('Correlation of English human judgements with Russian human judgements: {:0.2f} ({:0.2f})'.\n",
    "      format(*spearmanr(*make_mapped_datasets_en_ru(df_en, df_ru))))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of Russian human judgements with English human judgements: nan (nan)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_en = ew.copy()\n",
    "df_ru = rw.copy()\n",
    "\n",
    "print('Correlation of Russian human judgements with English human judgements: {:0.2f} ({:0.2f})'.\n",
    "     format(*spearmanr(*make_mapped_datasets_ru_en(df_ru, df_en))))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of dropped = 3.5%, amount of remanining words = 1348\n",
      "Percent of dropped = 36.9%, amount of remanining words = 851\n",
      "Correlation with Russian human judgements of English word2vec vectors: 0.32 (0.00)\n",
      "\n",
      "Correlation with Russian human judgements of Russian word2vec vectors: 0.37 (0.00)\n"
     ]
    }
   ],
   "source": [
    "df_ru = rw.copy()\n",
    "\n",
    "dataset = load_sim_dataset(df_ru, rusvectores, True, True)\n",
    "dataset.to_csv('temp.csv')\n",
    "dataset = load_en_sim_dataset(read_csv('temp.csv'), googlenews, True)\n",
    "\n",
    "w2v_en = make_en_sims_dataset(dataset, googlenews)\n",
    "print('Correlation with Russian human judgements of English word2vec vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(w2v_en, dataset.similarity)))\n",
    "print()\n",
    "\n",
    "w2v_ru = make_sims_dataset(dataset, rusvectores, rusvectores=True)\n",
    "print('Correlation with Russian human judgements of Russian word2vec vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(w2v_ru, dataset.similarity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of dropped = 84.0%, amount of remanining words = 1830\n",
      "Percent of dropped = 0.2%, amount of remanining words = 1826\n",
      "Correlation with English human judgements of Russian word2vec vectors: 0.24 (0.00)\n",
      "\n",
      "Correlation with Enlglish human judgements of English word2vec vectors: 0.39 (0.00)\n"
     ]
    }
   ],
   "source": [
    "df_en = ew.copy()\n",
    "\n",
    "dataset = load_ru_sim_dataset(df_en, rusvectores, True, rusvectores=True)\n",
    "dataset.to_csv('temp.csv')\n",
    "dataset = load_sim_dataset(read_csv('temp.csv'), googlenews, True)\n",
    "\n",
    "w2v_ru = make_ru_sims_dataset(dataset, rusvectores, rusvectores=True)\n",
    "print('Correlation with English human judgements of Russian word2vec vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(w2v_ru, dataset.similarity)))\n",
    "print()\n",
    "\n",
    "w2v_en = make_sims_dataset(dataset, googlenews)\n",
    "print('Correlation with Enlglish human judgements of English word2vec vectors: {:0.2f} ({:0.2f})'.format(*spearmanr(w2v_en, dataset.similarity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
