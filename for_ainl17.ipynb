{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_csv\n",
    "from csv import reader\n",
    "from numpy import mean, array, zeros, errstate\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load raw messy corpus proposed by the Laboratory of Neurolinguistics and transform it to a decent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join('data', 'data.csv'), mode='r') as infile:\n",
    "    corpus_bytes = reader(infile)\n",
    "    corpus = [i for i in corpus_bytes]\n",
    "\n",
    "corpus = [i[0].split('\\t') if len(i) == 1 else ''.join(i).split('\\t') for i in corpus]\n",
    "df = DataFrame(corpus[1:], columns=corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount of unique words in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['word.id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate corpus data by unqiue words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.groupby('Lemma').agg({\n",
    "                            'average.accuracy': ', '.join,\n",
    "                            'IA_DWELL_TIME': ', '.join,\n",
    "                            'IA_FIRST_FIXATION_DURATION': ', '.join,\n",
    "                            'IA_FIRST_RUN_DWELL_TIME': ', '.join,\n",
    "                            'IA_FIRST_RUN_FIXATION_COUNT': ', '.join,\n",
    "                            'IA_FIXATION_COUNT': ', '.join,\n",
    "                            'IA_LEGAL': ', '.join,\n",
    "                            'IA_REGRESSION_IN': ', '.join,\n",
    "                            'IA_REGRESSION_OUT_FULL': ', '.join,\n",
    "                            'IA_REGRESSION_PATH_DURATION': ', '.join,\n",
    "                            'IA_SECOND_RUN_DWELL_TIME': ', '.join,\n",
    "                            'ao': ', '.join,\n",
    "                            'IA_SELECTIVE_REGRESSION_PATH_DURATION': ', '.join,\n",
    "                            'IA_SKIP': ', '.join,\n",
    "                            'IA_SPILLOVER': ', '.join,\n",
    "                            'landing': ', '.join,\n",
    "                            'dir': ', '.join,\n",
    "                            'fixated.letter': ', '.join,\n",
    "                            'one_fix': ', '.join,\n",
    "                            'twoplus_fix': ', '.join\n",
    "                        }).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('data_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean value of aggregated values for each word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/defeater/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/defeater/anaconda/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "with errstate(divide='raise'):\n",
    "    for column in df:\n",
    "        for i in range(len(df)):\n",
    "            try:\n",
    "                values = list(filter(lambda a: a != 'NA', [a.strip() for a in df[column][i].split(',')]))\n",
    "                df[column][i] = mean([float(a) for a in values])\n",
    "            except ValueError: # if column is a column of words\n",
    "                continue\n",
    "            except FloatingPointError:\n",
    "                df[column][i] = None\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dictionary of obtained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_embeddings = defaultdict()\n",
    "\n",
    "for i, k in df.iterrows():\n",
    "    eye_embeddings[k['Lemma']] = array(k[1:].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and processing dataset of human judgements of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sim_dataset(name, verbose=False):\n",
    "    df = read_csv(path.join('data', '{}.csv'.format(name))).dropna()\n",
    "    old_len = len(df)\n",
    "    for i, m in df.iterrows():\n",
    "        if not m['word1'] in eye_embeddings or not m['word2'] in eye_embeddings:\n",
    "            df.drop(i, inplace=True)\n",
    "    if verbose:\n",
    "        print('Percent of dropped = {:2.1f}%'.format((old_len - len(df))/old_len*100))\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sims_dataset(dataset):\n",
    "    sims = zeros(shape=len(dataset), dtype='float32')\n",
    "    for i, m in dataset.iterrows():\n",
    "        sims[i] = 1 - cosine(eye_embeddings[m['word1']], eye_embeddings[m['word2']])\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HJ: Human Judgements of Word Pairs\n",
      "Percent of dropped = 97.2%\n",
      "Correlation: SpearmanrResult(correlation=0.1425321252592967, pvalue=0.67590154052978346)\n",
      "RT: Synonyms and Hypernyms from the Thesaurus RuThes\n",
      "Percent of dropped = 99.9%\n",
      "Correlation: SpearmanrResult(correlation=0.057346295562877694, pvalue=0.55179510954640909)\n",
      "AE: Cognitive Associations from the Sociation.org Experiment\n",
      "Percent of dropped = 99.7%\n",
      "Correlation: SpearmanrResult(correlation=-0.13052904375047683, pvalue=0.023288976503718622)\n"
     ]
    }
   ],
   "source": [
    "for name, printed_name in [\n",
    "             ('hj', 'HJ: Human Judgements of Word Pairs'),\n",
    "             ('rt', 'RT: Synonyms and Hypernyms from the Thesaurus RuThes'), \n",
    "             ('ae2', 'AE: Cognitive Associations from the Sociation.org Experiment'),\n",
    "            ]:\n",
    "    print(printed_name)\n",
    "    dataset = load_sim_dataset(name, True)\n",
    "    eye_sims = make_sims_dataset(dataset)\n",
    "    print('Correlation: {}'.format(spearmanr(eye_sims, dataset.sim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
