{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame, read_csv\n",
    "from csv import reader\n",
    "from numpy import mean, array, zeros, errstate, seterr\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr\n",
    "from os import path\n",
    "\n",
    "seterr(all='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load raw messy corpus proposed by the Laboratory of Neurolinguistics and transform it to a decent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path.join('data', 'data.csv'), mode='r') as infile:\n",
    "    corpus_bytes = reader(infile)\n",
    "    corpus = [i for i in corpus_bytes]\n",
    "\n",
    "corpus = [i[0].split('\\t') if len(i) == 1 else ''.join(i).split('\\t') for i in corpus]\n",
    "df = DataFrame(corpus[1:], columns=corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount of unique words in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['word.id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate corpus data by unqiue words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.groupby('Lemma').agg({\n",
    "                            'average.accuracy': ', '.join,\n",
    "                            'IA_DWELL_TIME': ', '.join,\n",
    "                            'IA_FIRST_FIXATION_DURATION': ', '.join,\n",
    "                            'IA_FIRST_RUN_DWELL_TIME': ', '.join,\n",
    "                            'IA_FIRST_RUN_FIXATION_COUNT': ', '.join,\n",
    "                            'IA_FIXATION_COUNT': ', '.join,\n",
    "                            'IA_LEGAL': ', '.join,\n",
    "                            'IA_REGRESSION_IN': ', '.join,\n",
    "                            'IA_REGRESSION_OUT_FULL': ', '.join,\n",
    "                            'IA_REGRESSION_PATH_DURATION': ', '.join,\n",
    "                            'IA_SECOND_RUN_DWELL_TIME': ', '.join,\n",
    "                            'ao': ', '.join,\n",
    "                            'IA_SELECTIVE_REGRESSION_PATH_DURATION': ', '.join,\n",
    "                            'IA_SKIP': ', '.join,\n",
    "                            'IA_SPILLOVER': ', '.join,\n",
    "                            'landing': ', '.join,\n",
    "                            'dir': ', '.join,\n",
    "                            'fixated.letter': ', '.join,\n",
    "                            'one_fix': ', '.join,\n",
    "                            'twoplus_fix': ', '.join\n",
    "                        }).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('data_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean value of aggregated values for each word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/defeater/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for column in df:\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            values = list(filter(lambda a: a != 'NA', [a.strip() for a in df[column][i].split(',')]))\n",
    "            df[column][i] = mean([float(a) for a in values])\n",
    "        except ValueError: # if column is a column of words\n",
    "            continue\n",
    "        except FloatingPointError: # invalid value encountered in double_scalars\n",
    "            df[column][i] = None\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and processing dataset of human judgements of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sim_dataset(name, embeddings, verbose=False):\n",
    "    df = read_csv(path.join('data', '{}.csv'.format(name))).dropna()\n",
    "    old_len = len(df)\n",
    "    for i, m in df.iterrows():\n",
    "        if not m['word1'] in embeddings or not m['word2'] in embeddings:\n",
    "            df.drop(i, inplace=True)\n",
    "    if verbose:\n",
    "        print('Percent of dropped = {:2.1f}%, amount of remanining words = {}'.format((old_len - len(df))/old_len*100, len(df)))\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sims_dataset(dataset, embeddings):\n",
    "    sims = zeros(shape=len(dataset), dtype='float32')\n",
    "    for i, m in dataset.iterrows():\n",
    "        sims[i] = 1 - cosine(embeddings[m['word1']], embeddings[m['word2']])\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20 features of the data\n",
      "RuSimLex999\n",
      "Percent of dropped = 97.8%, amount of remanining words = 22\n",
      "Correlation: SpearmanrResult(correlation=0.29072402840403871, pvalue=0.1893225753876453)\n",
      "\n",
      "RuSimLex965\n",
      "Percent of dropped = 97.8%, amount of remanining words = 21\n",
      "Correlation: SpearmanrResult(correlation=0.27633295828034554, pvalue=0.22529758979842845)\n",
      "\n",
      "HJ: Human Judgements of Word Pairs\n",
      "Percent of dropped = 97.2%, amount of remanining words = 11\n",
      "Correlation: SpearmanrResult(correlation=0.1425321252592967, pvalue=0.67590154052978346)\n",
      "\n",
      "RT: Synonyms and Hypernyms from the Thesaurus RuThes\n",
      "Percent of dropped = 99.9%, amount of remanining words = 110\n",
      "Correlation: SpearmanrResult(correlation=0.057346295562877694, pvalue=0.55179510954640909)\n",
      "\n",
      "AE: Cognitive Associations from the Sociation.org Experiment\n",
      "Percent of dropped = 99.7%, amount of remanining words = 302\n",
      "Correlation: SpearmanrResult(correlation=-0.13052904375047683, pvalue=0.023288976503718622)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "STARTING_AMOUNT = 20\n",
    "\n",
    "for columns_amount in range(STARTING_AMOUNT,len(df.columns)):\n",
    "    eye_embeddings = defaultdict()\n",
    "    for i, k in df.ix[:,0:columns_amount].iterrows():\n",
    "        eye_embeddings[k['Lemma']] = array(k[1:].values)\n",
    "    print('Using {} features of the data'.format(columns_amount))\n",
    "    for name, printed_name in [\n",
    "                ('simlex999', 'RuSimLex999'),\n",
    "                ('simlex965', 'RuSimLex965'),\n",
    "                ('hj', 'HJ: Human Judgements of Word Pairs'),\n",
    "                ('rt', 'RT: Synonyms and Hypernyms from the Thesaurus RuThes'), \n",
    "                ('ae2', 'AE: Cognitive Associations from the Sociation.org Experiment'),\n",
    "                ]:\n",
    "        print('{}'.format(printed_name))\n",
    "        dataset = load_sim_dataset(name, eye_embeddings, True)\n",
    "        try:\n",
    "            eye_sims = make_sims_dataset(dataset, eye_embeddings)\n",
    "        except FloatingPointError: # invalid value encountered in double_scalars\n",
    "            eye_sims = zeros(shape=len(dataset), dtype='float32')\n",
    "            print('NaN')\n",
    "            print()\n",
    "            continue\n",
    "        print('Correlation: {}'.format(spearmanr(eye_sims, dataset.sim)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
