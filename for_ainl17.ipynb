{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'invalid': 'ignore', 'over': 'ignore', 'under': 'ignore'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame, read_csv\n",
    "from csv import reader\n",
    "from numpy import mean, array, zeros, errstate, seterr\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr\n",
    "from os import path\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "seterr(all='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load raw messy corpus proposed by the Laboratory of Neurolinguistics and transform it to a decent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path.join('data', 'data.csv'), mode='r') as infile:\n",
    "    corpus_bytes = reader(infile)\n",
    "    corpus = [i for i in corpus_bytes]\n",
    "\n",
    "corpus = [i[0].split('\\t') if len(i) == 1 else ''.join(i).split('\\t') for i in corpus]\n",
    "df = DataFrame(corpus[1:], columns=corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount of unique words in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['word.id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate corpus data by unqiue words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.groupby('Lemma').agg({\n",
    "                            'average.accuracy': ', '.join,\n",
    "                            'IA_DWELL_TIME': ', '.join,\n",
    "                            'IA_FIRST_FIXATION_DURATION': ', '.join,\n",
    "                            'IA_FIRST_RUN_DWELL_TIME': ', '.join,\n",
    "                            'IA_FIRST_RUN_FIXATION_COUNT': ', '.join,\n",
    "                            'IA_FIXATION_COUNT': ', '.join,\n",
    "                            'IA_LEGAL': ', '.join,\n",
    "                            'IA_REGRESSION_IN': ', '.join,\n",
    "                            'IA_REGRESSION_OUT_FULL': ', '.join,\n",
    "                            'IA_REGRESSION_PATH_DURATION': ', '.join,\n",
    "                            'IA_SECOND_RUN_DWELL_TIME': ', '.join,\n",
    "                            'ao': ', '.join,\n",
    "                            'IA_SELECTIVE_REGRESSION_PATH_DURATION': ', '.join,\n",
    "                            'IA_SKIP': ', '.join,\n",
    "                            'IA_SPILLOVER': ', '.join,\n",
    "                            'landing': ', '.join,\n",
    "                            'dir': ', '.join,\n",
    "                            'fixated.letter': ', '.join,\n",
    "                            'one_fix': ', '.join,\n",
    "                            'twoplus_fix': ', '.join\n",
    "                        }).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('data_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean value of aggregated values for each word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/defeater/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for column in df:\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            values = list(filter(lambda a: a != 'NA', [a.strip() for a in df[column][i].split(',')]))\n",
    "            df[column][i] = mean([float(a) for a in values])\n",
    "        except ValueError: # if column is a column of words\n",
    "            continue\n",
    "        except FloatingPointError: # invalid value encountered in double_scalars\n",
    "            df[column][i] = None\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and processing dataset of human judgements of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sim_dataset(name, embeddings, verbose=False):\n",
    "    df = read_csv(path.join('data', '{}.csv'.format(name))).dropna()\n",
    "    old_len = len(df)\n",
    "    for i, m in df.iterrows():\n",
    "        if not m['word1'] in embeddings or not m['word2'] in embeddings:\n",
    "            df.drop(i, inplace=True)\n",
    "    if verbose:\n",
    "        print('Percent of dropped = {:2.1f}%, amount of remanining words = {}'.format((old_len - len(df))/old_len*100, len(df)))\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sims_dataset(dataset, embeddings):\n",
    "    sims = zeros(shape=len(dataset), dtype='float32')\n",
    "    for i, m in dataset.iterrows():\n",
    "        sims[i] = 1 - cosine(embeddings[m['word1']], embeddings[m['word2']])\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_word2vec_dataset(dataset, model):\n",
    "    sims = zeros(shape=len(dataset), dtype='float32')\n",
    "    for i, m in dataset.iterrows():\n",
    "        sims[i] = 1 - cosine(model[add_pos_tag(m['word1'])], model[add_pos_tag(m['word2'])])\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rusvectores = KeyedVectors.load_word2vec_format(path.join('..', 'models', 'rusvectores.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pos_tag(word):\n",
    "    tag = str(morph.parse(word)[0].tag.POS)\n",
    "    if tag == 'ADJF':\n",
    "        tag = 'ADJ'\n",
    "    elif tag == 'INFN':\n",
    "        tag = 'VERB'\n",
    "    if word == 'объем': # sorry\n",
    "        tag = 'NOUN'\n",
    "    if word == 'струя':\n",
    "        tag = 'NOUN'\n",
    "    if word == 'чай':\n",
    "        tag = 'NOUN'\n",
    "    if word == 'два':\n",
    "        word = 'двадцать'\n",
    "        tag = 'NUM'\n",
    "    return '{}_{}'.format(word, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/defeater/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20 features of the data\n",
      "RuSimLex999\n",
      "Percent of dropped = 97.8%, amount of remanining words = 22\n",
      "Correlation with human judgements: SpearmanrResult(correlation=0.29072402840403871, pvalue=0.1893225753876453)\n",
      "Correlation of human judmenets with Word2Vec: SpearmanrResult(correlation=0.20248872017246275, pvalue=0.36613882836713274)\n",
      "Correlation With word2Vec: SpearmanrResult(correlation=0.28773318258903335, pvalue=0.19412392123350533)\n",
      "\n",
      "RuSimLex965\n",
      "Percent of dropped = 97.8%, amount of remanining words = 21\n",
      "Correlation with human judgements: SpearmanrResult(correlation=0.27633295828034554, pvalue=0.22529758979842845)\n",
      "Correlation of human judmenets with Word2Vec: SpearmanrResult(correlation=0.27698315347629932, pvalue=0.2241633525290902)\n",
      "Correlation With word2Vec: SpearmanrResult(correlation=0.38531513970110459, pvalue=0.084537153629326484)\n",
      "\n",
      "HJ: Human Judgements of Word Pairs\n",
      "Percent of dropped = 97.2%, amount of remanining words = 11\n",
      "Correlation with human judgements: SpearmanrResult(correlation=0.1425321252592967, pvalue=0.67590154052978346)\n",
      "Correlation of human judmenets with Word2Vec: SpearmanrResult(correlation=0.6023131744828345, pvalue=0.049882380531143973)\n",
      "Correlation With word2Vec: SpearmanrResult(correlation=0.21461187214611874, pvalue=0.52627545113089957)\n",
      "\n",
      "RT: Synonyms and Hypernyms from the Thesaurus RuThes\n",
      "Percent of dropped = 99.9%, amount of remanining words = 110\n",
      "Correlation with human judgements: SpearmanrResult(correlation=0.057346295562877694, pvalue=0.55179510954640909)\n",
      "Correlation of human judmenets with Word2Vec: SpearmanrResult(correlation=0.3395853407363329, pvalue=0.00028409546478450878)\n",
      "Correlation With word2Vec: SpearmanrResult(correlation=0.31954053967376639, pvalue=0.00066710579731831064)\n",
      "\n",
      "AE: Cognitive Associations from the Sociation.org Experiment\n",
      "Percent of dropped = 99.7%, amount of remanining words = 302\n",
      "Correlation with human judgements: SpearmanrResult(correlation=-0.13052904375047683, pvalue=0.023288976503718622)\n",
      "Correlation of human judmenets with Word2Vec: SpearmanrResult(correlation=0.41917549210338317, pvalue=2.7942989627494542e-14)\n",
      "Correlation With word2Vec: SpearmanrResult(correlation=0.042244537314265196, pvalue=0.46452635387189201)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "STARTING_AMOUNT = 20\n",
    "\n",
    "for columns_amount in range(STARTING_AMOUNT,len(df.columns)):\n",
    "    eye_embeddings = defaultdict()\n",
    "    for i, k in df.ix[:,0:columns_amount].iterrows():\n",
    "        eye_embeddings[k['Lemma']] = array(k[1:].values)\n",
    "    print('Using {} features of the data'.format(columns_amount))\n",
    "    for name, printed_name in [\n",
    "                ('simlex999', 'RuSimLex999'),\n",
    "                ('simlex965', 'RuSimLex965'),\n",
    "                ('hj', 'HJ: Human Judgements of Word Pairs'),\n",
    "                ('rt', 'RT: Synonyms and Hypernyms from the Thesaurus RuThes'), \n",
    "                ('ae2', 'AE: Cognitive Associations from the Sociation.org Experiment'),\n",
    "                ]:\n",
    "        print('{}'.format(printed_name))\n",
    "        dataset = load_sim_dataset(name, eye_embeddings, True)\n",
    "        try:\n",
    "            eye_sims = make_sims_dataset(dataset, eye_embeddings)\n",
    "            word2vec_sims = make_word2vec_dataset(dataset, rusvectores)\n",
    "        except FloatingPointError: # invalid value encountered in double_scalars\n",
    "            eye_sims = zeros(shape=len(dataset), dtype='float32')\n",
    "            print('NaN')\n",
    "            print()\n",
    "            continue\n",
    "        print('Correlation with human judgements: {}'.format(spearmanr(eye_sims, dataset.sim)))\n",
    "        print('Correlation of human judmenets with Word2Vec: {}'.format(spearmanr(word2vec_sims, dataset.sim)))\n",
    "        print('Correlation With word2Vec: {}'.format(spearmanr(word2vec_sims, eye_sims)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
